import os
import collections
import nltk
from nltk.corpus import stopwords
os.chdir(r"D:\pythonProjects\txtForNlp")


os.getcwd()


os.listdir()


with open("shelock.txt") as f:
    text = f.read()
nltk.download("punkt")


tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')


sentence_nltk = tokenizer.tokenize(text)


print(sentence_nltk[2])


words =nltk.word_tokenize(text.lower())
from collections import Counter
words = [w for w in words if w.isalpha()]

nltk.download("stopwords")



stop_words = set(stopwords.words('english'))
filtered = [word for word in words if word not in stop_words]
count_words = Counter(filtered)
print(filtered[:50])



print(count_words.most_common(75))


print(count_words.get("god"))


from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemitzied = [lemmatizer.lemmatize(word) for word in filtered]






for orig, lemma in list(zip(filtered, lemitzied))[50:100]:
    print(f"{orig}  â†’  {lemma}")






mostcommon30 = count_words.most_common(30)


print(mostcommon30)


words, freqs = zip(*mostcommon30)



import matplotlib.pyplot as plt
plt.figure(figsize=(10,6))
plt.bar(words, freqs)
plt.xticks(rotation=45)
plt.title("Top 20 Most Common Words")
plt.xlabel("Words")
plt.ylabel("Frequency")
plt.show()


import sys
print(sys.executable)




from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))

filtered_counts = [(w, c) for w, c in count_words.most_common(200) if w.lower() not in stop_words]
words, freqs = zip(*filtered_counts[:20])

plt.figure(figsize=(10,6))
plt.bar(words, freqs)
plt.xticks(rotation=45)
plt.title("Top 20 Content Words (Stopwords Removed)")
plt.show()




